<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Introduction to machine learning" />
    <meta name="keywords" content="Machine Learning" />
    <meta name="author" content="Avrabos Georgios" />
    <link rel="stylesheet" href="styles.css" />
    <title>
      InfoTuah | Intro to ML | Introduction to Machine Learning — Dmitry Kobak,
      2020/21
    </title>
  </head>
  <body>
    <!-- Navbar -->
    <header>
      <nav class="navbar">
        <div class="nav-image-wrapper">
          <a href="index.html">
            <img src="public/Info_Tuah_Logo_Final.png" alt="InfoTuahLogo" />
          </a>
        </div>
        <ul>
          <li>
            <a href="Courses.html">Courses</a>
          </li>
          <li>
            <a href="about_us.html">About Us</a>
          </li>
        </ul>
      </nav>
    </header>

    <main>
      <div class="main-container">
        <!-- Return Link -->
        <a href="ai_intro_to_ml.html">
          <svg
            xmlns="http://www.w3.org/2000/svg"
            height="24px"
            viewBox="0 -960 960 960"
            width="24px"
            fill="#5f6368"
          >
            <path
              d="M280-200v-80h284q63 0 109.5-40T720-420q0-60-46.5-100T564-560H312l104 104-56 56-200-200 200-200 56 56-104 104h252q97 0 166.5 63T800-420q0 94-69.5 157T564-200H280Z"
            />
          </svg>
          Return To Subcourse
        </a>

        <!-- Lecture Title -->
        <h1>Introduction to Machine Learning — Dmitry Kobak, 2020/21</h1>
        <section class="lecture-info">
          <!-- Lecture Description, Contributors, Likes-->
          <div class="lecture-description">
            <span>
              The course serves as a basic introduction to machine learning and
              covers key concepts in regression, classification, optimization,
              regularization, clustering, and dimensionality reduction. The
              course is aimed at master students in neuroscience and other
              sciences. The course assumes basic familiarity with calculus,
              probability theory, and linear algebra (matrices).
            </span>
            <div class="lecture-contributors">
              <p>Contributors:</p>
              <span>Dmitry Kobak</span>
            </div>
            <div class="lecture-likes">
              <span>954</span>
              <svg
                xmlns="http://www.w3.org/2000/svg"
                height="24px"
                viewBox="0 -960 960 960"
                width="24px"
                fill="#5f6368"
              >
                <path
                  d="M720-120H280v-520l280-280 50 50q7 7 11.5 19t4.5 23v14l-44 174h258q32 0 56 24t24 56v80q0 7-2 15t-4 15L794-168q-9 20-30 34t-44 14Zm-360-80h360l120-280v-80H480l54-220-174 174v406Zm0-406v406-406Zm-80-34v80H160v360h120v80H80v-520h200Z"
                />
              </svg>
            </div>
          </div>

          <!-- Lecture Unit Table-->
          <div
            class="lecture-unit-table"
            id="ai_intro_to_ml_lec3_lecture_unit_table"
          >
            <h2>Unit table for this Lecture</h2>
            <ol>
              <li>
                <a href="#ai_intro_to_ml_lec3_vid1"
                  >01 - Baby steps towards linear regression</a
                >
              </li>
              <li>
                <a href="#ai_intro_to_ml_lec3_vid2"
                  >02 - Multiple linear regression and SVD</a
                >
              </li>
              <li>
                <a href="#ai_intro_to_ml_lec3_vid3"
                  >03 - Likelihood, bias, and variance</a
                >
              </li>
              <li>
                <a href="#ai_intro_to_ml_lec3_vid4"
                  >04 - Regularization and cross-validation</a
                >
              </li>
              <li>
                <a href="#ai_intro_to_ml_lec3_vid5">05 - Logistic regression</a>
              </li>
              <li>
                <a href="#ai_intro_to_ml_lec3_vid6"
                  >06 - Linear discriminant analysis</a
                >
              </li>
              <li>
                <a href="#ai_intro_to_ml_lec3_vid7"
                  >07 - Neural networks and deep learning</a
                >
              </li>
              <li>
                <a href="#ai_intro_to_ml_lec3_vid8"
                  >08 - Boosting, bagging, and random forests</a
                >
              </li>
              <li>
                <a href="#ai_intro_to_ml_lec3_vid9"
                  >09 - Clustering and expectation-maximization</a
                >
              </li>
              <li>
                <a href="#ai_intro_to_ml_lec3_vid10"
                  >10 - Principal component analysis</a
                >
              </li>
              <li>
                <a href="#ai_intro_to_ml_lec3_vid11"
                  >11 - Manifold learning and t-SNE</a
                >
              </li>
            </ol>
          </div>
        </section>

        <!-- Lecture Videos -->
        <section class="lecture-videos">
          <h2>Lectures</h2>
          <hr />
          <ol>
            <li id="ai_intro_to_ml_lec3_vid1">
              <div class="lecture-video-wrapper">
                <iframe
                  width="640"
                  height="360"
                  src="https://www.youtube.com/embed/lWGdFeMsjzg?list=PL05umP7R6ij35ShKLDqccJSDntugY4FQT"
                  title="Introduction to Machine Learning - 01 - Baby steps towards linear regression"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
              <div class="lecture-video-info">
                <span>01 - Baby steps towards linear regression</span>
                <span>
                  In this video, we provide an Introduction to Machine Learning,
                  covering foundational concepts such as the definition of
                  machine learning and how it differs from traditional
                  problem-solving. You'll learn about the Types of Machine
                  Learning Problems—supervised, unsupervised, and reinforcement
                  learning—before diving into Linear Regression as a starting
                  point. We also discuss Optimization techniques like Gradient
                  Descent and extend these ideas to Simple Linear Regression,
                  explaining how to use partial derivatives for
                  multi-dimensional optimization.
                </span>
                <div class="lecture-video-return-to-ut">
                  <a href="#ai_intro_to_ml_lec3_lecture_unit_table">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      height="24px"
                      viewBox="0 -960 960 960"
                      width="24px"
                      fill="#5f6368"
                    >
                      <path
                        d="M440-160v-487L216-423l-56-57 320-320 320 320-56 57-224-224v487h-80Z"
                      />
                    </svg>
                  </a>
                </div>
              </div>
            </li>
            <li id="ai_intro_to_ml_lec3_vid2">
              <div class="lecture-video-wrapper">
                <iframe
                  width="640"
                  height="360"
                  src="https://www.youtube.com/embed/FaGjn9eZIVE"
                  title="Introduction to Machine Learning - 02 - Multiple linear regression and SVD"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
              <div class="lecture-video-info">
                <span>02 - Multiple linear regression and SVD</span>
                <span>
                  In this video, we explore advanced concepts in Multiple
                  Regression, starting with how regression can include multiple
                  predictors and the visualization of relationships as a
                  regression plane in higher dimensions. We introduce the matrix
                  formulation to simplify calculations and discuss the loss
                  function (mean squared error) that guides optimization. Key
                  techniques like Gradient Descent and Singular Value
                  Decomposition (SVD) are explained for efficient optimization
                  and ensuring numerical stability in regression models with
                  correlated predictors.
                </span>
                <div class="lecture-video-return-to-ut">
                  <a href="#ai_intro_to_ml_lec3_lecture_unit_table">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      height="24px"
                      viewBox="0 -960 960 960"
                      width="24px"
                      fill="#5f6368"
                    >
                      <path
                        d="M440-160v-487L216-423l-56-57 320-320 320 320-56 57-224-224v487h-80Z"
                      />
                    </svg>
                  </a>
                </div>
              </div>
            </li>
            <li id="ai_intro_to_ml_lec3_vid3">
              <div class="lecture-video-wrapper">
                <iframe
                  width="640"
                  height="360"
                  src="https://www.youtube.com/embed/brkS6rAKTl4"
                  title="Introduction to Machine Learning - 03 - Likelihood, bias, and variance"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
              <div class="lecture-video-info">
                <span>03 - Likelihood, bias, and variance</span>
                <span>
                  In this video, we introduce probabilistic models in linear
                  regression and explore the importance of likelihood in
                  estimating parameters. Key concepts such as the bias-variance
                  trade-off and the effects of overfitting and underfitting on
                  model performance are discussed. We also highlight the role of
                  sample size in model flexibility, the significance of the
                  Gaussian distribution, and explain how Maximum Likelihood
                  Estimation is used for parameter fitting.
                </span>
                <div class="lecture-video-return-to-ut">
                  <a href="#ai_intro_to_ml_lec3_lecture_unit_table">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      height="24px"
                      viewBox="0 -960 960 960"
                      width="24px"
                      fill="#5f6368"
                    >
                      <path
                        d="M440-160v-487L216-423l-56-57 320-320 320 320-56 57-224-224v487h-80Z"
                      />
                    </svg>
                  </a>
                </div>
              </div>
            </li>
            <li id="ai_intro_to_ml_lec3_vid4">
              <div class="lecture-video-wrapper">
                <iframe
                  width="640"
                  height="360"
                  src="https://www.youtube.com/embed/9ku_jgTyC4Y"
                  title="Introduction to Machine Learning - 04 - Regularization and cross-validation"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
              <div class="lecture-video-info">
                <span>04 - Regularization and cross-validation</span>
                <span>
                  In this video, we explore techniques to improve model
                  performance, starting with regularization to prevent
                  overfitting by penalizing large coefficients. We cover the
                  importance of cross-validation in evaluating models and
                  discuss the bias-variance trade-off for model selection.
                  You’ll learn about Ridge and Lasso regularization, their
                  properties, and how Elastic Net combines both techniques.
                  Additionally, we explain the role of hyperparameter tuning for
                  optimal performance and the use of nested cross-validation for
                  robust model validation.
                </span>
                <div class="lecture-video-return-to-ut">
                  <a href="#ai_intro_to_ml_lec3_lecture_unit_table">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      height="24px"
                      viewBox="0 -960 960 960"
                      width="24px"
                      fill="#5f6368"
                    >
                      <path
                        d="M440-160v-487L216-423l-56-57 320-320 320 320-56 57-224-224v487h-80Z"
                      />
                    </svg>
                  </a>
                </div>
              </div>
            </li>
            <li id="ai_intro_to_ml_lec3_vid5">
              <div class="lecture-video-wrapper">
                <iframe
                  width="640"
                  height="360"
                  src="https://www.youtube.com/embed/cGUjiHyQWPk"
                  title="Introduction to Machine Learning - 05 - Logistic regression"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
              <div class="lecture-video-info">
                <span>05 - Logistic regression</span>
                <span>
                  In this video, we transition from regression to classification
                  with an introduction to logistic regression. You'll learn how
                  logistic regression handles binary and multi-class
                  classification and why linear regression is unsuitable for
                  classification tasks. We explain how logistic regression
                  predicts probabilities and the difference in its loss
                  function, focusing on maximizing likelihood. The video also
                  covers the convexity of the loss function for efficient
                  optimization and discusses the bias-variance trade-off,
                  addressing overfitting and regularization challenges.
                </span>
                <div class="lecture-video-return-to-ut">
                  <a href="#ai_intro_to_ml_lec3_lecture_unit_table">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      height="24px"
                      viewBox="0 -960 960 960"
                      width="24px"
                      fill="#5f6368"
                    >
                      <path
                        d="M440-160v-487L216-423l-56-57 320-320 320 320-56 57-224-224v487h-80Z"
                      />
                    </svg>
                  </a>
                </div>
              </div>
            </li>
            <li id="ai_intro_to_ml_lec3_vid6">
              <div class="lecture-video-wrapper">
                <iframe
                  width="640"
                  height="360"
                  src="https://www.youtube.com/embed/C0u_v7vEDBY"
                  title="Introduction to Machine Learning - 06 - Linear discriminant analysis"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
              <div class="lecture-video-info">
                <span>06 - Linear discriminant analysis</span>
                <span>
                  Discover how LDA, a linear classification method, models data
                  distribution given a class, unlike logistic regression which
                  models class probability. Learn about its assumption of
                  Gaussian distributions for each class, leading to linear
                  decision boundaries when class covariances are equal. Explore
                  how regularization helps mitigate overfitting, and compare LDA
                  with alternatives like the nearest centroid classifier and
                  non-parametric methods such as K-Nearest Neighbors.
                </span>
                <div class="lecture-video-return-to-ut">
                  <a href="#ai_intro_to_ml_lec3_lecture_unit_table">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      height="24px"
                      viewBox="0 -960 960 960"
                      width="24px"
                      fill="#5f6368"
                    >
                      <path
                        d="M440-160v-487L216-423l-56-57 320-320 320 320-56 57-224-224v487h-80Z"
                      />
                    </svg>
                  </a>
                </div>
              </div>
            </li>
            <li id="ai_intro_to_ml_lec3_vid7">
              <div class="lecture-video-wrapper">
                <iframe
                  width="640"
                  height="360"
                  src="https://www.youtube.com/embed/Uf9xJDJt1KA"
                  title="Introduction to Machine Learning - 07 - Neural networks and deep learning"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
              <div class="lecture-video-info">
                <span>07 - Neural networks and deep learning</span>
                <span>
                  Dive into deep learning, where computers learn complex
                  concepts from simpler ones. Discover the significant successes
                  of neural networks in tasks like image classification, thanks
                  to their hierarchical organization for efficient pattern
                  recognition. Understand how backpropagation efficiently
                  computes gradients for training, and how Convolutional Neural
                  Networks (CNNs) use weight sharing to detect image features.
                  Learn about the balance between overfitting and generalization
                  in neural networks, and explore ongoing research into
                  understanding their deeper layers.
                </span>
                <div class="lecture-video-return-to-ut">
                  <a href="#ai_intro_to_ml_lec3_lecture_unit_table">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      height="24px"
                      viewBox="0 -960 960 960"
                      width="24px"
                      fill="#5f6368"
                    >
                      <path
                        d="M440-160v-487L216-423l-56-57 320-320 320 320-56 57-224-224v487h-80Z"
                      />
                    </svg>
                  </a>
                </div>
              </div>
            </li>
            <li id="ai_intro_to_ml_lec3_vid8">
              <div class="lecture-video-wrapper">
                <iframe
                  width="640"
                  height="360"
                  src="https://www.youtube.com/embed/thR9ncsyMBE"
                  title="Introduction to Machine Learning - 08 - Boosting, bagging, and random forests"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
              <div class="lecture-video-info">
                <span>08 - Boosting, bagging, and random forests</span>
                <span>
                  Explore how boosting reduces bias by building complex models
                  from simple ones, while bagging combines multiple models to
                  reduce variance and improve performance. Understand the role
                  of classification trees as the foundation for both techniques
                  and the importance of managing the bias-variance trade-off.
                  Learn about Adaboost, a popular boosting algorithm, and how
                  random forests enhance model diversity through random feature
                  selection. Discover how both methods achieve high training
                  accuracy while maintaining good test performance.
                </span>
                <div class="lecture-video-return-to-ut">
                  <a href="#ai_intro_to_ml_lec3_lecture_unit_table">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      height="24px"
                      viewBox="0 -960 960 960"
                      width="24px"
                      fill="#5f6368"
                    >
                      <path
                        d="M440-160v-487L216-423l-56-57 320-320 320 320-56 57-224-224v487h-80Z"
                      />
                    </svg>
                  </a>
                </div>
              </div>
            </li>
            <li id="ai_intro_to_ml_lec3_vid9">
              <div class="lecture-video-wrapper">
                <iframe
                  width="640"
                  height="360"
                  src="https://www.youtube.com/embed/mU3GZaOoVDA"
                  title="Introduction to Machine Learning - 09 - Clustering and expectation-maximization"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
              <div class="lecture-video-info">
                <span>09 - Clustering and expectation-maximization</span>
                <span>
                  Transition from supervised to unsupervised learning concepts,
                  focusing on clustering problems to identify groups in
                  unlabeled datasets. Understand the k-means algorithm and its
                  iterative optimization process, and learn about the
                  Expectation-Maximization (EM) algorithm for Gaussian mixture
                  models. Differentiate between hard cluster assignment in
                  k-means and soft assignment in Gaussian mixtures. Discuss
                  challenges like local minima and initialization in clustering
                  algorithms, and recognize the limitations of k-means and
                  Gaussian mixtures in clustering complex shapes.
                </span>
                <div class="lecture-video-return-to-ut">
                  <a href="#ai_intro_to_ml_lec3_lecture_unit_table">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      height="24px"
                      viewBox="0 -960 960 960"
                      width="24px"
                      fill="#5f6368"
                    >
                      <path
                        d="M440-160v-487L216-423l-56-57 320-320 320 320-56 57-224-224v487h-80Z"
                      />
                    </svg>
                  </a>
                </div>
              </div>
            </li>
            <li id="ai_intro_to_ml_lec3_vid10">
              <div class="lecture-video-wrapper">
                <iframe
                  width="640"
                  height="360"
                  src="https://www.youtube.com/embed/xBf_LZ5ZgY4"
                  title="Introduction to Machine Learning - 10 - Principal component analysis"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
              <div class="lecture-video-info">
                <span>10 - Principal component analysis</span>
                <span>
                  Learn how PCA reduces dimensionality by transforming features
                  into a smaller set of uncorrelated variables, aiding in data
                  visualization and exploratory analysis. Discover how PCA
                  minimizes reconstruction error and maximizes variance, with
                  the first principal component capturing the largest variance
                  direction. Understand its use in preprocessing to improve
                  supervised learning model performance, and explore
                  Probabilistic PCA for a latent variable model perspective.
                  Grasp key concepts like eigenvectors and eigenvalues essential
                  for understanding PCA’s functionality.
                </span>
                <div class="lecture-video-return-to-ut">
                  <a href="#ai_intro_to_ml_lec3_lecture_unit_table">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      height="24px"
                      viewBox="0 -960 960 960"
                      width="24px"
                      fill="#5f6368"
                    >
                      <path
                        d="M440-160v-487L216-423l-56-57 320-320 320 320-56 57-224-224v487h-80Z"
                      />
                    </svg>
                  </a>
                </div>
              </div>
            </li>
            <li id="ai_intro_to_ml_lec3_vid11">
              <div class="lecture-video-wrapper">
                <iframe
                  width="640"
                  height="360"
                  src="https://www.youtube.com/embed/MnRskV3NY1k"
                  title="Introduction to Machine Learning - 11 - Manifold learning and t-SNE"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                ></iframe>
              </div>
              <div class="lecture-video-info">
                <span>11 - Manifold learning and t-SNE</span>
                <span>
                  Discover t-SNE and its crucial role in visualizing
                  high-dimensional data through dimensionality reduction
                  techniques. Learn about its applications in biology, including
                  single-cell transcriptomics and population genetics, and the
                  importance of maintaining data structure in visualizations.
                  Understand the attraction and repulsion forces in t-SNE
                  optimization, and gain insights on parameter tuning, such as
                  perplexity and early exaggeration. Explore the future
                  potential of combining continuous and discrete data structures
                  in embeddings.
                </span>
                <div class="lecture-video-return-to-ut">
                  <a href="#ai_intro_to_ml_lec3_lecture_unit_table">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      height="24px"
                      viewBox="0 -960 960 960"
                      width="24px"
                      fill="#5f6368"
                    >
                      <path
                        d="M440-160v-487L216-423l-56-57 320-320 320 320-56 57-224-224v487h-80Z"
                      />
                    </svg>
                  </a>
                </div>
              </div>
            </li>
          </ol>
        </section>
      </div>
    </main>

    <!-- Footer -->
    <footer>
      <div class="footer-contents">
        <div class="footer-links">
          <div class="footer-contact-us">
            <!-- Github Icon -->
            <a href="https://github.com/Gsprx/InfoTuah" target="_blank"
              ><?xml version="1.0"?><svg
                xmlns="http://www.w3.org/2000/svg"
                viewBox="0 0 50 50"
                width="28px"
                height="28px"
              >
                <path
                  d="M17.791,46.836C18.502,46.53,19,45.823,19,45v-5.4c0-0.197,0.016-0.402,0.041-0.61C19.027,38.994,19.014,38.997,19,39 c0,0-3,0-3.6,0c-1.5,0-2.8-0.6-3.4-1.8c-0.7-1.3-1-3.5-2.8-4.7C8.9,32.3,9.1,32,9.7,32c0.6,0.1,1.9,0.9,2.7,2c0.9,1.1,1.8,2,3.4,2 c2.487,0,3.82-0.125,4.622-0.555C21.356,34.056,22.649,33,24,33v-0.025c-5.668-0.182-9.289-2.066-10.975-4.975 c-3.665,0.042-6.856,0.405-8.677,0.707c-0.058-0.327-0.108-0.656-0.151-0.987c1.797-0.296,4.843-0.647,8.345-0.714 c-0.112-0.276-0.209-0.559-0.291-0.849c-3.511-0.178-6.541-0.039-8.187,0.097c-0.02-0.332-0.047-0.663-0.051-0.999 c1.649-0.135,4.597-0.27,8.018-0.111c-0.079-0.5-0.13-1.011-0.13-1.543c0-1.7,0.6-3.5,1.7-5c-0.5-1.7-1.2-5.3,0.2-6.6 c2.7,0,4.6,1.3,5.5,2.1C21,13.4,22.9,13,25,13s4,0.4,5.6,1.1c0.9-0.8,2.8-2.1,5.5-2.1c1.5,1.4,0.7,5,0.2,6.6c1.1,1.5,1.7,3.2,1.6,5 c0,0.484-0.045,0.951-0.11,1.409c3.499-0.172,6.527-0.034,8.204,0.102c-0.002,0.337-0.033,0.666-0.051,0.999 c-1.671-0.138-4.775-0.28-8.359-0.089c-0.089,0.336-0.197,0.663-0.325,0.98c3.546,0.046,6.665,0.389,8.548,0.689 c-0.043,0.332-0.093,0.661-0.151,0.987c-1.912-0.306-5.171-0.664-8.879-0.682C35.112,30.873,31.557,32.75,26,32.969V33 c2.6,0,5,3.9,5,6.6V45c0,0.823,0.498,1.53,1.209,1.836C41.37,43.804,48,35.164,48,25C48,12.318,37.683,2,25,2S2,12.318,2,25 C2,35.164,8.63,43.804,17.791,46.836z"
                />
              </svg>
            </a>
            <!-- Instagram Icon -->
            <a href="https://www.instagram.com/" target="_blank">
              <svg
                xmlns="http://www.w3.org/2000/svg"
                viewBox="0 0 50 50"
                width="28px"
                height="28px"
              >
                <path
                  d="M 16 3 C 8.8324839 3 3 8.8324839 3 16 L 3 34 C 3 41.167516 8.8324839 47 16 47 L 34 47 C 41.167516 47 47 41.167516 47 34 L 47 16 C 47 8.8324839 41.167516 3 34 3 L 16 3 z M 16 5 L 34 5 C 40.086484 5 45 9.9135161 45 16 L 45 34 C 45 40.086484 40.086484 45 34 45 L 16 45 C 9.9135161 45 5 40.086484 5 34 L 5 16 C 5 9.9135161 9.9135161 5 16 5 z M 37 11 A 2 2 0 0 0 35 13 A 2 2 0 0 0 37 15 A 2 2 0 0 0 39 13 A 2 2 0 0 0 37 11 z M 25 14 C 18.936712 14 14 18.936712 14 25 C 14 31.063288 18.936712 36 25 36 C 31.063288 36 36 31.063288 36 25 C 36 18.936712 31.063288 14 25 14 z M 25 16 C 29.982407 16 34 20.017593 34 25 C 34 29.982407 29.982407 34 25 34 C 20.017593 34 16 29.982407 16 25 C 16 20.017593 20.017593 16 25 16 z"
                />
              </svg>
            </a>
            <!-- LinkedIn Icon -->
            <a href="https://www.linkedin.com" target="_blank"
              ><?xml version="1.0"?><svg
                xmlns="http://www.w3.org/2000/svg"
                viewBox="0 0 50 50"
                width="28px"
                height="28px"
              >
                <path
                  d="M41,4H9C6.24,4,4,6.24,4,9v32c0,2.76,2.24,5,5,5h32c2.76,0,5-2.24,5-5V9C46,6.24,43.76,4,41,4z M17,20v19h-6V20H17z M11,14.47c0-1.4,1.2-2.47,3-2.47s2.93,1.07,3,2.47c0,1.4-1.12,2.53-3,2.53C12.2,17,11,15.87,11,14.47z M39,39h-6c0,0,0-9.26,0-10 c0-2-1-4-3.5-4.04h-0.08C27,24.96,26,27.02,26,29c0,0.91,0,10,0,10h-6V20h6v2.56c0,0,1.93-2.56,5.81-2.56 c3.97,0,7.19,2.73,7.19,8.26V39z"
                />
              </svg>
            </a>
            <!-- Twiter Icon -->
            <a href="https://x.com" target="_blank">
              <svg
                xmlns="http://www.w3.org/2000/svg"
                viewBox="0 0 50 50"
                width="28px"
                height="28px"
              >
                <path
                  d="M 5.9199219 6 L 20.582031 27.375 L 6.2304688 44 L 9.4101562 44 L 21.986328 29.421875 L 31.986328 44 L 44 44 L 28.681641 21.669922 L 42.199219 6 L 39.029297 6 L 27.275391 19.617188 L 17.933594 6 L 5.9199219 6 z M 9.7167969 8 L 16.880859 8 L 40.203125 42 L 33.039062 42 L 9.7167969 8 z"
                />
              </svg>
            </a>
          </div>
          <div class="footer-privacy-terms">
            <a href="privacy_policy.html">Privacy Policy</a>
            <a href="terms_of_service.html">Terms of Service</a>
          </div>
        </div>
        <div class="footer-text">
          <p>
            Empowering learners with cutting-edge computer science education.
          </p>
          <p>
            InfoTuah is designed to enhance learning and skill-building, with
            simplified examples to support better readability and understanding.
            Tutorials, references, and examples are regularly reviewed for
            accuracy, though we cannot guarantee that all content is error-free.
            By using InfoTuah, you acknowledge that you have read and agreed to
            our terms of use and privacy policy.
          </p>
        </div>
        <div class="footer-copyright">
          Copyright ©2024-2025 InfoTuah. All rights reserved.
        </div>
      </div>
    </footer>
  </body>
</html>
